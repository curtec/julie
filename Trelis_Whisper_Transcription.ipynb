{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curtec/julie/blob/main/Trelis_Whisper_Transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qzwD9ts4_kc"
      },
      "source": [
        "<font size=64px>Whisper Transcription</font>\n",
        "\n",
        "Notebook by Trelis Research. Built upon an [original notebook](https://colab.research.google.com/github/deepgram-devs/try-whisper-in-google-collab/blob/main/try_whisper_in_three_easy_steps.ipynb) by Ross O'Connell.\n",
        "\n",
        "Find:\n",
        "- [Trelis on YouTube](https://youtube.com/@trelisresearch)\n",
        "- [The Trelis Newsletter here](https://blog.trelis.com).\n",
        "- [Fine-tuning video and scripts - upcoming]()\n",
        "\n",
        "\n",
        "-\n",
        "\n",
        "Key updates since original Whisper Fine-tuning video:\n",
        "- Allow transcription from uploaded mp3, mp4 or wav files.\n",
        "- Make whisper-turbo the default for lower word error rate and 2x speed up versus whisper small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biPhaxG7LHPj"
      },
      "source": [
        "## Mount Google Drive (optional)\n",
        "You later need to change the paths below for pulling and transcribing audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KKivchsPXCC"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdeaEucBPYim"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# cache_dir = \"/content/drive/My Drive/video_transcripts\"\n",
        "# os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tATxgEBmLK_R"
      },
      "source": [
        "## Install Whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1QHcVQz4Gu7"
      },
      "source": [
        "In the first line we install Whisper!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOvKw2K3kWqK",
        "outputId": "4218a6e8-c0ec-4d7b-e62a-f056f2013d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe uploaded mp3 or wav or m4a files\n",
        "Upload your files to on the left hand pane (or, adjust the path below if you have mounted Drive)."
      ],
      "metadata": {
        "id": "ei55zdLoL-BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the first mp3, wav, or m4a file in the /content/ directory\n",
        "audio_files = [file for file in os.listdir('/content') if file.endswith(('.mp3', '.wav', '.m4a'))]\n",
        "\n",
        "if audio_files:\n",
        "    audio_file = f\"/content/{audio_files[0]}\" # convert the first audio file\n",
        "    # Call whisper with the dynamically found audio file (MP3, WAV, or M4A)\n",
        "    os.system(f'whisper \"{audio_file}\" --model small --language English')\n",
        "else:\n",
        "    print(\"No MP3, WAV, or M4A file found in the /content/ directory.\")"
      ],
      "metadata": {
        "id": "wnuO62xkL9pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FOR A CUSTOM MODEL\n",
        "\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "import subprocess\n",
        "\n",
        "# Download the model file from the Hugging Face repo\n",
        "repo_id = \"Trelis/whisper-turbo-llm-lingo\"\n",
        "model_file = \"whisper-turbo-llm-lingo-openai.bin\"\n",
        "\n",
        "# Download the model file from Hugging Face\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=model_file)\n",
        "\n",
        "# Get the first mp3, wav, or m4a file in the /content/ directory\n",
        "audio_files = [file for file in os.listdir('/content') if file.endswith(('.mp3', '.wav', '.m4a'))]\n",
        "\n",
        "if audio_files:\n",
        "    audio_file = f\"/content/{audio_files[0]}\" # convert the first audio file\n",
        "    # Call whisper with the dynamically found audio file (MP3, WAV, or M4A)\n",
        "    # Pass the local model path instead of a predefined model name\n",
        "    command = f'whisper \"{audio_file}\" --model \"{model_path}\" --language English'\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "else:\n",
        "    print(\"No MP3, WAV, or M4A file found in the /content/ directory.\")"
      ],
      "metadata": {
        "id": "nO76SpNDiT03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-BeZRNmVlX",
        "outputId": "855180e3-72b4-4992-c4e2-267134041b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--Trelis--whisper-turbo-llm-lingo/snapshots/db89ed6efa18c99b36bf5bdb41a1fc9519ae68dc/whisper-turbo-llm-lingo-openai.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FatSKi3YAQCx"
      },
      "source": [
        "### Grab and transcribe audio from YouTube\n",
        "Note that downloading from YouTube can be difficult outside of Colab and may require you to authenticate with YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NqzXPOXQ6JQ",
        "outputId": "c85c08f3-2146-4b1e-d2fe-53676984ff9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.3/171.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYaGfY1J2VRi"
      },
      "outputs": [],
      "source": [
        "# !yt-dlp https://youtube.com/video/VUKZP0ShxEM --format m4a -o \"/content/%(id)s.%(ext)s\"\n",
        "# !whisper \"/content/VUKZP0ShxEM.m4a\" --model small --language English"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# User hardcodes the YouTube link\n",
        "youtube_link = \"https://youtube.com/watch?v=VUKZP0ShxEM\"\n",
        "\n",
        "# Extract video ID from the YouTube link\n",
        "video_id = youtube_link.split(\"v=\")[-1]\n",
        "\n",
        "# Use yt-dlp to download the audio as m4a format\n",
        "os.system(f'yt-dlp {youtube_link} --format m4a -o \"/content/{video_id}.%(ext)s\"')\n",
        "\n",
        "# The file path for the downloaded m4a file\n",
        "audio_file = f\"/content/{video_id}.m4a\"\n",
        "\n",
        "# Run Whisper for transcription on the downloaded audio file\n",
        "os.system(f'whisper \"{audio_file}\" --model turbo --language English')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3lAbt2pNTar",
        "outputId": "ff868445-316b-4b28-f4de-086eb35d7b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## For a custom model\n",
        "\n",
        "import os, subprocess\n",
        "\n",
        "# User hardcodes the YouTube link\n",
        "youtube_link = \"https://youtube.com/watch?v=VUKZP0ShxEM\"\n",
        "\n",
        "# Extract video ID from the YouTube link\n",
        "video_id = youtube_link.split(\"v=\")[-1]\n",
        "\n",
        "# Use yt-dlp to download the audio as m4a format\n",
        "os.system(f'yt-dlp {youtube_link} --format m4a -o \"/content/{video_id}.%(ext)s\"')\n",
        "\n",
        "# The file path for the downloaded m4a file\n",
        "audio_file = f\"/content/{video_id}.m4a\"\n",
        "\n",
        "# Download the model file from the Hugging Face repo\n",
        "repo_id = \"Trelis/whisper-turbo-llm-lingo\"\n",
        "model_file = \"whisper-turbo-llm-lingo-openai.bin\"\n",
        "\n",
        "# Download the model file from Hugging Face\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=model_file)\n",
        "\n",
        "# Run Whisper for transcription on the downloaded audio file\n",
        "command = f'whisper \"{audio_file}\" --model \"{model_path}\" --language English'\n",
        "errors = subprocess.run(command, shell=True, capture_output=True, text=True)"
      ],
      "metadata": {
        "id": "INmn7-NUOl8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To see any errors, if any\n",
        "# print(errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hov4LQx3nzEX",
        "outputId": "39abcec2-3ce1-42d4-af7a-71040d92386a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompletedProcess(args='whisper \"/content/VUKZP0ShxEM.m4a\" --model \"/root/.cache/huggingface/hub/models--Trelis--whisper-turbo-llm-lingo/snapshots/db89ed6efa18c99b36bf5bdb41a1fc9519ae68dc/whisper-turbo-llm-lingo-openai.bin\" --language English', returncode=0, stdout='Skipping /content/VUKZP0ShxEM.m4a due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\\n  libavutil      56. 70.100 / 56. 70.100\\n  libavcodec     58.134.100 / 58.134.100\\n  libavformat    58. 76.100 / 58. 76.100\\n  libavdevice    58. 13.100 / 58. 13.100\\n  libavfilter     7.110.100 /  7.110.100\\n  libswscale      5.  9.100 /  5.  9.100\\n  libswresample   3.  9.100 /  3.  9.100\\n  libpostproc    55.  9.100 / 55.  9.100\\n/content/VUKZP0ShxEM.m4a: No such file or directory\\n\\n', stderr='/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don\\'t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\\n  checkpoint = torch.load(fp, map_location=device)\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\\n    out = run(cmd, capture_output=True, check=True).stdout\\n  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\\n    raise CalledProcessError(retcode, process.args,\\nsubprocess.CalledProcessError: Command \\'[\\'ffmpeg\\', \\'-nostdin\\', \\'-threads\\', \\'0\\', \\'-i\\', \\'/content/VUKZP0ShxEM.m4a\\', \\'-f\\', \\'s16le\\', \\'-ac\\', \\'1\\', \\'-acodec\\', \\'pcm_s16le\\', \\'-ar\\', \\'16000\\', \\'-\\']\\' returned non-zero exit status 1.\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\\n    result = transcribe(model, audio_path, temperature=temperature, **args)\\n  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 133, in transcribe\\n    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\\n  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\\n    audio = load_audio(audio)\\n  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\\n    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\\nRuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\\n  libavutil      56. 70.100 / 56. 70.100\\n  libavcodec     58.134.100 / 58.134.100\\n  libavformat    58. 76.100 / 58. 76.100\\n  libavdevice    58. 13.100 / 58. 13.100\\n  libavfilter     7.110.100 /  7.110.100\\n  libswscale      5.  9.100 /  5.  9.100\\n  libswresample   3.  9.100 /  3.  9.100\\n  libpostproc    55.  9.100 / 55.  9.100\\n/content/VUKZP0ShxEM.m4a: No such file or directory\\n\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VP14RDwCoMI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}